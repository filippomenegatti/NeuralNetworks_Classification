{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "name": "ML_assignment.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "lQj2x2U1AeLm",
        "LkEoRHoWoaqR",
        "eBrHUH5ABz7-",
        "2vF3pouC6awN"
      ],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/filippomenegatti/NeuralNetworks_Classification/blob/main/Script_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "surprising-arena"
      },
      "source": [
        "# Image classification with Neural Networks\n",
        "Use `tensorflow` to train neural networks for the classification of fruit/vegetable types based on images from this dataset. Images must be transformed from JPG to RGB pixel values and scaled down (e.g., 32x32).  \n",
        "Use fruit/vegetable types (as opposed to variety) as labels to predict and consider only the 10 most frequent types (apple, banana, plum, pepper, cherry, grape, tomato, potato, pear, peach).  \n",
        "Experiment with different network architectures and training parameters documenting their influence of the final predictive performance. While the training loss can be chosen freely, the reported test errors must be measured according to the zero-one loss for multiclass classification."
      ],
      "id": "surprising-arena"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPGqDSbLtHjF"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "In these essay we are going to analyse the dataset available on Kaggle website [1] under the license CC BY-SA 4.0, using a Deep Learning approach. The dataset contains 90380 images of 131 fruits and vegetables divided in folders for training and test set respectively. We are going to select just a subsample of the available fruits creating 10 macrocategories with the most frequent types. Different Neural Networks architectures will be compared, starting from different settings of the Feedforward Neural Networks and concluding with two Convolutional Neural Network models.\n",
        "\n"
      ],
      "id": "pPGqDSbLtHjF"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMW1FQJJPHL6"
      },
      "source": [
        "# Setting up the environment"
      ],
      "id": "KMW1FQJJPHL6"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZkHiGu_Gu12"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "OZkHiGu_Gu12",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYn4zn5hkL77"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.upload() #import the kaggle.json file"
      ],
      "id": "qYn4zn5hkL77",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZChZTy2kLzb"
      },
      "source": [
        "#install kaggle and download the data set in the desired path\n",
        "!pip install -q kaggle\n",
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d moltean/fruits\n",
        "!mkdir ML_assignment\n",
        "!unzip fruits.zip -d ML_assignment"
      ],
      "id": "qZChZTy2kLzb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "serious-privilege"
      },
      "source": [
        "#import all the libraries and functions\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, KFold\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.preprocessing.image import array_to_img, img_to_array, load_img\n",
        "from keras.utils import np_utils\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Flatten, Dropout, Conv2D, MaxPool2D, Activation, MaxPooling2D, Input, AveragePooling2D, GlobalAveragePooling2D\n",
        "from keras.losses import categorical_crossentropy\n",
        "from keras.optimizers import Adam, SGD, RMSprop, Adamax\n",
        "from keras import regularizers\n",
        "from keras.callbacks import LearningRateScheduler, History, EarlyStopping\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "print('The Tensorflow version used is: ' + tf.__version__)\n",
        "print('The Keras version used is: ' + keras.__version__)"
      ],
      "id": "serious-privilege",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "underlying-australian"
      },
      "source": [
        "#set the seed\n",
        "seed = 33\n",
        "random.seed(seed)\n",
        "tf.random.set_seed(seed)"
      ],
      "id": "underlying-australian",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkEoRHoWoaqR"
      },
      "source": [
        "# Dataset preprocessing"
      ],
      "id": "LkEoRHoWoaqR"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "breeding-flooring"
      },
      "source": [
        "# import of the dataset divided in the 10 categories requested with the target size of 32x32\n",
        "\n",
        "types = [\"Apple\", \"Banana\", \"Plum\", \"Pepper\", \"Cherry\", \"Grape\", \"Tomato\", \"Potato\", \"Pear\", \"Peach\"]\n",
        "\n",
        "fruits = {}\n",
        "\n",
        "def load_dataset(dire):\n",
        "    fruits = {}\n",
        "    images_as_array = []\n",
        "    labels = []\n",
        "    for category in os.listdir(dire):\n",
        "        for typ in types:\n",
        "            if(category.split()[0] == typ):\n",
        "                fruits[category]= typ\n",
        "                path = os.path.join(dire,category)\n",
        "                class_num =types.index(fruits[category])\n",
        "\n",
        "                class_name = fruits[category]\n",
        "                for img in os.listdir(path):\n",
        "                    file = os.path.join(path,img)\n",
        "                    images_as_array.append(img_to_array(load_img(file,target_size=(32, 32))))\n",
        "                    labels.append(class_num)\n",
        "    images_as_array =  np.array(images_as_array)\n",
        "    labels = np.array(labels)\n",
        "    return images_as_array, labels"
      ],
      "id": "breeding-flooring",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cubic-worthy"
      },
      "source": [
        "train_path= '/content/ML_assignment/fruits-360/Training'\n",
        "test_path= '/content/ML_assignment/fruits-360/Test'\n",
        "train = load_dataset(train_path)\n",
        "test = load_dataset(test_path)\n",
        "X_train, y_train = train\n",
        "X_test, y_test = test\n",
        "X_train, y_train = shuffle(X_train, y_train)\n",
        "X_test, y_test = shuffle(X_test, y_test)\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)"
      ],
      "id": "cubic-worthy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "simplified-jamaica"
      },
      "source": [
        "n_classes = len(np.unique(y_train))\n",
        "print(n_classes)"
      ],
      "id": "simplified-jamaica",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "referenced-helicopter"
      },
      "source": [
        "#look at the distribution of the classes in the sets to see if they are balanced_\n",
        "\n",
        "unique_train, counts_train = np.unique(y_train, return_counts=True)\n",
        "plt.bar(unique_train, counts_train)\n",
        "\n",
        "unique_test, counts_test = np.unique(y_test, return_counts=True)\n",
        "plt.bar(unique_test, counts_test)\n",
        "\n",
        "plt.xticks(rotation=45)\n",
        "plt.gca().legend(('y_train','y_test'))\n",
        "plt.title('Class Frequency')\n",
        "plt.xlabel('Class')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "plt.show()"
      ],
      "id": "referenced-helicopter",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "widespread-joshua"
      },
      "source": [
        "# creation the validation set as a 20% of the training one\n",
        "X_val, X_train, y_val, y_train  = train_test_split(X_train, y_train, train_size = 0.20)"
      ],
      "id": "widespread-joshua",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "palestinian-wallpaper"
      },
      "source": [
        "# normalization of the sets\n",
        "X_train = X_train.astype('float32')/255\n",
        "X_test = X_test.astype('float32')/255\n",
        "X_val = X_val.astype('float32')/255\n",
        "\n",
        "print('Training X:\\n',X_train.shape)\n",
        "print('\\nVaildation X:\\n',X_val.shape)\n",
        "print('\\nTest X:\\n',X_test.shape)"
      ],
      "id": "palestinian-wallpaper",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "signed-graphics"
      },
      "source": [
        "# image example of the data\n",
        "\n",
        "n_rows = 3\n",
        "n_cols = 6\n",
        "plt.figure(figsize=(n_cols * 1.5, n_rows * 1.5))\n",
        "for row in range(n_rows):\n",
        "    for col in range(n_cols):\n",
        "        index = n_cols * row + col\n",
        "        plt.subplot(n_rows, n_cols, index + 1)\n",
        "        plt.imshow(X_train[index], cmap=\"binary\", interpolation=\"nearest\")\n",
        "        plt.axis('off')\n",
        "        plt.title(types[y_train[index]], fontsize=12)\n",
        "plt.subplots_adjust(wspace=0.2, hspace=0.5)\n",
        "plt.show()"
      ],
      "id": "signed-graphics",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lovely-health"
      },
      "source": [
        "# convert labels to categorical\n",
        "y_train = np_utils.to_categorical(y_train, n_classes)\n",
        "y_val = np_utils.to_categorical(y_val, n_classes)\n",
        "y_test = np_utils.to_categorical(y_test, n_classes)"
      ],
      "id": "lovely-health",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rough-mason"
      },
      "source": [
        "# definition of the zero-one loss function used for the calculation of the test error\n",
        "def zo_loss(test, pred):\n",
        "    y_hat = []\n",
        "    y_t = []\n",
        "    for i in range(len(pred)):\n",
        "        y_hat.append(np.argmax(pred[i]))\n",
        "        y_t.append(np.argmax(test[i]))\n",
        "    loss = []\n",
        "    for i in range(len(pred)):\n",
        "        if(y_hat[i] == y_t[i]):\n",
        "            loss.append(0)\n",
        "        else:\n",
        "            loss.append(1)\n",
        "    return np.mean(loss)"
      ],
      "id": "rough-mason",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "supposed-married"
      },
      "source": [
        "# Feedforward Deep Neural Networks"
      ],
      "id": "supposed-married"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFVDkMKsfNmy"
      },
      "source": [
        "### First basic model"
      ],
      "id": "XFVDkMKsfNmy"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qualified-florence"
      },
      "source": [
        "model1 = keras.Sequential()\n",
        "model1.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
        "model1.add(keras.layers.Dense(1000, activation=\"relu\"))\n",
        "model1.add(keras.layers.Dense(400, activation=\"relu\"))\n",
        "model1.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
        "\n",
        "model1.compile(loss = keras.losses.categorical_crossentropy,\n",
        "              optimizer = \"sgd\",\n",
        "              metrics = [\"accuracy\"])\n",
        "\n",
        "model1.summary()"
      ],
      "id": "qualified-florence",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "infrared-event"
      },
      "source": [
        "%%time\n",
        "\n",
        "history1 = model1.fit(X_train, y_train, epochs=30,\n",
        "                    validation_data=(X_val, y_val), \n",
        "                    verbose = 1, \n",
        "                    callbacks = [EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True)]\n",
        "                    )"
      ],
      "id": "infrared-event",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atomic-developer"
      },
      "source": [
        "model1.evaluate(X_train, y_train)\n",
        "model1.evaluate(X_test, y_test)"
      ],
      "id": "atomic-developer",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "funky-soundtrack"
      },
      "source": [
        "pd.DataFrame(history1.history).plot(figsize=(8, 5))\n",
        "plt.grid(True)\n",
        "plt.gca().set_ylim(0, 1)\n",
        "plt.show()"
      ],
      "id": "funky-soundtrack",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "veterinary-factor"
      },
      "source": [
        "y_pred1 = model1.predict(X_test)\n",
        "zo_loss(y_test, y_pred1)"
      ],
      "id": "veterinary-factor",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scug2uPpeBwq"
      },
      "source": [
        "### Nesterov and exponential decay"
      ],
      "id": "scug2uPpeBwq"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cognitive-barbados"
      },
      "source": [
        "# define the learning rate change \n",
        "def exp_decay(epoch):\n",
        "    lrate = learning_rate * np.exp(-decay_rate*epoch)\n",
        "    return lrate\n",
        "early_stop = EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True)\n",
        "epochs = 30\n",
        "decay_rate = 1e-6\n",
        "momentum = 0.9\n",
        "learning_rate = 0.01\n",
        "sgd = SGD(lr=learning_rate, momentum=momentum, decay=decay_rate, nesterov=True)\n",
        "loss_history = History()\n",
        "lr_rate = LearningRateScheduler(exp_decay)\n",
        "callbacks_list = [loss_history, lr_rate, early_stop]"
      ],
      "id": "cognitive-barbados",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "careful-leadership"
      },
      "source": [
        "model2 = keras.Sequential()\n",
        "model2.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
        "model2.add(keras.layers.Dense(1000, activation=\"relu\"))\n",
        "model2.add(keras.layers.Dense(400, activation=\"relu\"))\n",
        "model2.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
        "\n",
        "model2.compile(loss = keras.losses.categorical_crossentropy,\n",
        "              optimizer = sgd,\n",
        "              metrics = [\"accuracy\"])\n",
        "\n",
        "model2.summary()"
      ],
      "id": "careful-leadership",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "several-figure"
      },
      "source": [
        "%%time\n",
        "\n",
        "model2_history = model2.fit(X_train, y_train, epochs=epochs, \n",
        "                            verbose=1, callbacks=callbacks_list,\n",
        "                            validation_data=(X_val, y_val))"
      ],
      "id": "several-figure",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGYXeqP36B3Y"
      },
      "source": [
        "pd.DataFrame(model2_history.history).plot(figsize=(8, 5))\n",
        "plt.grid(True)\n",
        "plt.gca().set_ylim(0, 1)\n",
        "plt.show()"
      ],
      "id": "hGYXeqP36B3Y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ongoing-leather"
      },
      "source": [
        "model2.evaluate(X_train, y_train)\n",
        "model2.evaluate(X_test, y_test)"
      ],
      "id": "ongoing-leather",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vocal-certificate"
      },
      "source": [
        "y_pred2 = model2.predict(X_test)\n",
        "zo_loss(y_test, y_pred2)"
      ],
      "id": "vocal-certificate",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moved-arabic"
      },
      "source": [
        "### Dropout  "
      ],
      "id": "moved-arabic"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unique-underground"
      },
      "source": [
        "model3 = keras.Sequential()\n",
        "model3.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
        "model3.add(keras.layers.Dense(1000, activation=\"relu\"))\n",
        "model3.add(keras.layers.Dropout(0.1))\n",
        "model3.add(keras.layers.Dense(400, activation=\"relu\"))\n",
        "model3.add(keras.layers.Dropout(0.2))\n",
        "model3.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
        "\n",
        "model3.compile(loss = keras.losses.categorical_crossentropy,\n",
        "              optimizer = sgd,\n",
        "              metrics = [\"accuracy\"])\n",
        "\n",
        "model3.summary()"
      ],
      "id": "unique-underground",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "correct-flavor"
      },
      "source": [
        "%%time\n",
        "\n",
        "history3 = model3.fit(X_train, y_train, epochs=epochs, \n",
        "                            verbose=1, callbacks=callbacks_list,\n",
        "                            validation_data=(X_val, y_val))"
      ],
      "id": "correct-flavor",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fifty-drill"
      },
      "source": [
        "print(model3.evaluate(X_train, y_train))\n",
        "print(model3.evaluate(X_test, y_test))"
      ],
      "id": "fifty-drill",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "assisted-procedure"
      },
      "source": [
        "pd.DataFrame(history3.history).plot(figsize=(8, 5))\n",
        "plt.grid(True)\n",
        "plt.gca().set_ylim(0, 1)\n",
        "plt.show()\n",
        "\n",
        "y_pred3 = model3.predict(X_test)\n",
        "zo_loss(y_test, y_pred3)"
      ],
      "id": "assisted-procedure",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lzq8TOtIeRZK"
      },
      "source": [
        "### L1 and L2 regularizers"
      ],
      "id": "Lzq8TOtIeRZK"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unsigned-washer"
      },
      "source": [
        "model4 = keras.Sequential()\n",
        "model4.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
        "model4.add(keras.layers.Dense(1000, activation=\"relu\", kernel_regularizer=regularizers.l1_l2()))\n",
        "model4.add(keras.layers.Dense(400, activation=\"relu\", kernel_regularizer=regularizers.l1_l2()))\n",
        "model4.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
        "\n",
        "model4.compile(loss = keras.losses.categorical_crossentropy,\n",
        "              optimizer = sgd,\n",
        "              metrics = [\"accuracy\"])\n",
        "\n",
        "model4.summary()"
      ],
      "id": "unsigned-washer",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "married-replacement"
      },
      "source": [
        "%%time\n",
        "\n",
        "history4 = model4.fit(X_train, y_train, epochs=epochs, \n",
        "                            verbose=1, callbacks=callbacks_list,\n",
        "                            validation_data=(X_val, y_val))"
      ],
      "id": "married-replacement",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "challenging-annotation"
      },
      "source": [
        "pd.DataFrame(history4.history).plot(figsize=(8, 5))\n",
        "plt.grid(True)\n",
        "plt.gca().set_ylim(0, 1)\n",
        "plt.show()\n",
        "\n",
        "print(model4.evaluate(X_train, y_train))\n",
        "print(model4.evaluate(X_test, y_test))\n",
        "\n",
        "y_pred4 = model4.predict(X_test)\n",
        "print(zo_loss(y_test, y_pred4))"
      ],
      "id": "challenging-annotation",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGSv4bUVxhk4"
      },
      "source": [
        "# Hyperparameters Tuning"
      ],
      "id": "cGSv4bUVxhk4"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xa6j1Vz1xgO3"
      },
      "source": [
        "def create_model(optimizer = 'adam'):\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
        "    model.add(Dense(1000, activation=tf.nn.relu))\n",
        "    model.add(Dense(400, activation=tf.nn.relu))\n",
        "    model.add(Dense(10, activation=tf.nn.softmax))\n",
        " \n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=optimizer,\n",
        "              metrics=['accuracy'])\n",
        "    return model"
      ],
      "id": "Xa6j1Vz1xgO3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYq1VfWa7SXl"
      },
      "source": [
        "epochs = 30\n",
        "\n",
        "model_CV = KerasClassifier(build_fn=create_model, epochs=epochs, verbose=1)\n",
        "\n",
        "# define the grid search parameters\n",
        "\n",
        "optimizer = ['adam', 'rmsprop', 'adamax', 'nadam']\n",
        "\n",
        "param_grid = dict(optimizer=optimizer)\n",
        "grid = GridSearchCV(estimator=model_CV, param_grid=param_grid, cv=5)\n",
        "grid_result = grid.fit(X_train, y_train, callbacks=callbacks_list,\n",
        "                            validation_data=(X_val, y_val))"
      ],
      "id": "RYq1VfWa7SXl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGOYs8XT7tej"
      },
      "source": [
        "# print results\n",
        "print(f'Best Accuracy for {grid_result.best_score_} using {grid_result.best_params_}')\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(f' mean={mean:.5}, std={stdev:.5} using {param}')"
      ],
      "id": "LGOYs8XT7tej",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12s_uqHMbImT"
      },
      "source": [
        "def create_model_SGD(nl1=1, nl2=1,  nl3=1, \n",
        "                 nn1=200, nn2=100, nn3 = 50, l1=0.01, l2=0.01,\n",
        "                 dropout=0, output_shape=10, opt = sgd, act = 'relu'):\n",
        "    \n",
        "    \n",
        "    reg = keras.regularizers.l1_l2(l1=l1, l2=l2)\n",
        "                                                     \n",
        "    model = Sequential()\n",
        "    model.add(Flatten(input_shape=[32, 32, 3]))\n",
        "\n",
        "    first=True\n",
        "    \n",
        "    for i in range(nl1):\n",
        "        if first:\n",
        "            model.add(Dense(nn1, activation=act, kernel_regularizer=reg))\n",
        "            first=False\n",
        "        else: \n",
        "            model.add(Dense(nn1, activation=act, kernel_regularizer=reg))\n",
        "        if dropout!=0:\n",
        "            model.add(Dropout(dropout))\n",
        "            \n",
        "    for i in range(nl2):\n",
        "        if first:\n",
        "            model.add(Dense(nn2, activation=act, kernel_regularizer=reg))\n",
        "            first=False\n",
        "        else: \n",
        "            model.add(Dense(nn2, activation=act, kernel_regularizer=reg))\n",
        "        if dropout!=0:\n",
        "            model.add(Dropout(dropout))\n",
        "            \n",
        "    for i in range(nl3):\n",
        "        if first:\n",
        "            model.add(Dense(nn3, activation=act, kernel_regularizer=reg))\n",
        "            first=False\n",
        "        else: \n",
        "            model.add(Dense(nn3, activation=act, kernel_regularizer=reg)) \n",
        "        if dropout!=0:\n",
        "            model.add(Dropout(dropout))\n",
        "            \n",
        "    model.add(Dense(output_shape, activation='softmax'))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer= opt, metrics=['accuracy'],)\n",
        "    return model"
      ],
      "id": "12s_uqHMbImT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNFHTMRecIH1"
      },
      "source": [
        "model_SGD = KerasClassifier(build_fn=create_model_SGD, epochs=30, verbose=1)"
      ],
      "id": "RNFHTMRecIH1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_j_FA4kcH9v"
      },
      "source": [
        "# numbers of layers\n",
        "nl1 = [0,1,2,3]\n",
        "nl2 = [0,1,2,3]\n",
        "nl3 = [0,1,2,3]\n",
        "\n",
        "# neurons in each layer\n",
        "nn1=[1000, 1500, 2000,]\n",
        "nn2=[500,1000,1500]\n",
        "nn3=[250,500,1000]\n",
        "\n",
        "# dropout and regularisation\n",
        "dropout = [0, 0.1, 0.2, 0.3]\n",
        "l1 = [0, 0.01, 0.003, 0.001,0.0001]\n",
        "l2 = [0, 0.01, 0.003, 0.001,0.0001]\n",
        "\n",
        "# dictionary summary\n",
        "param_grid = dict(nl1=nl1, nl2=nl2, nl3=nl3, nn1=nn1, nn2=nn2, nn3=nn3, \n",
        "                  l1=l1, l2=l2, dropout=dropout)"
      ],
      "id": "l_j_FA4kcH9v",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XB3JQfffpa3"
      },
      "source": [
        "grid1 = RandomizedSearchCV(estimator=model_SGD, cv=KFold(5), param_distributions=param_grid, \n",
        "                          verbose=20,  n_iter=10)"
      ],
      "id": "7XB3JQfffpa3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_97P1A3af4ux"
      },
      "source": [
        "grid_result_SGD = grid1.fit(X_train, y_train, \n",
        "                            verbose=1, callbacks=callbacks_list,\n",
        "                            validation_data=(X_val, y_val))"
      ],
      "id": "_97P1A3af4ux",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdJOowBBVscu"
      },
      "source": [
        "grid_result_SGD.best_params_"
      ],
      "id": "gdJOowBBVscu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XrlhghdK3cFZ"
      },
      "source": [
        "best_SGD = grid_result_SGD.best_estimator_"
      ],
      "id": "XrlhghdK3cFZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dT6ZOY3NDui8"
      },
      "source": [
        "best_SGD.model.save(\"/content/drive/MyDrive/ML_NN/sgd\")"
      ],
      "id": "dT6ZOY3NDui8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjFHBbGms6E4"
      },
      "source": [
        "tunedSGD = keras.models.load_model(\"/content/drive/MyDrive/ML_NN/sgd\")"
      ],
      "id": "LjFHBbGms6E4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9PiljcccNnR"
      },
      "source": [
        "tunedSGD.summary()"
      ],
      "id": "M9PiljcccNnR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTfabAlh_rLV"
      },
      "source": [
        "historySGD = tunedSGD.fit(X_train, y_train,\n",
        "  verbose=1, callbacks=callbacks_list,\n",
        "  validation_data=(X_val, y_val),\n",
        "  epochs = 30)"
      ],
      "id": "jTfabAlh_rLV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wl8MLyTvvMAK"
      },
      "source": [
        "tunedSGD.evaluate(X_train, y_train)\n",
        "tunedSGD.evaluate(X_test, y_test)"
      ],
      "id": "Wl8MLyTvvMAK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJ-k1MWm_JBS"
      },
      "source": [
        "pd.DataFrame(historySGD.history).plot(figsize=(8, 5))\n",
        "plt.grid(True)\n",
        "plt.gca().set_ylim(0, 1)\n",
        "plt.show()"
      ],
      "id": "FJ-k1MWm_JBS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0o55Dx_IvMAM"
      },
      "source": [
        "y_pred = tunedSGD.predict(X_test)\n",
        "print(zo_loss(y_test, y_pred))"
      ],
      "id": "0o55Dx_IvMAM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4AfS6nmCIDm"
      },
      "source": [
        "def create_model_AM(nl1=1, nl2=1,  nl3=1, \n",
        "                 nn1=200, nn2=100, nn3 = 50, l1=0.01, l2=0.01,\n",
        "                 dropout=0, output_shape=10, opt = keras.optimizers.Adamax(), act = 'relu'):\n",
        "    \n",
        "    \n",
        "    reg = keras.regularizers.l1_l2(l1=l1, l2=l2)\n",
        "                                                     \n",
        "    model = Sequential()\n",
        "    model.add(Flatten(input_shape=[32, 32, 3]))\n",
        "\n",
        "    first=True\n",
        "    \n",
        "    for i in range(nl1):\n",
        "        if first:\n",
        "            model.add(Dense(nn1, activation=act, kernel_regularizer=reg)) #, kernel_initializer= init\n",
        "            first=False\n",
        "        else: \n",
        "            model.add(Dense(nn1, activation=act, kernel_regularizer=reg)) #, kernel_initializer= init\n",
        "        if dropout!=0:\n",
        "            model.add(Dropout(dropout))\n",
        "            \n",
        "    for i in range(nl2):\n",
        "        if first:\n",
        "            model.add(Dense(nn2, activation=act, kernel_regularizer=reg)) #, kernel_initializer= init\n",
        "            first=False\n",
        "        else: \n",
        "            model.add(Dense(nn2, activation=act, kernel_regularizer=reg)) #, kernel_initializer= init\n",
        "        if dropout!=0:\n",
        "            model.add(Dropout(dropout))\n",
        "            \n",
        "    for i in range(nl3):\n",
        "        if first:\n",
        "            model.add(Dense(nn3, activation=act, kernel_regularizer=reg)) #, kernel_initializer= init\n",
        "            first=False\n",
        "        else: \n",
        "            model.add(Dense(nn3, activation=act, kernel_regularizer=reg)) #, kernel_initializer= init\n",
        "        if dropout!=0:\n",
        "            model.add(Dropout(dropout))\n",
        "            \n",
        "    model.add(Dense(output_shape, activation='softmax'))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer= opt, metrics=['accuracy'],)\n",
        "    return model"
      ],
      "id": "A4AfS6nmCIDm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6jj16cQPYn-"
      },
      "source": [
        "model_Adamax = KerasClassifier(build_fn=create_model_AM, epochs=30, verbose=1)"
      ],
      "id": "m6jj16cQPYn-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3udy1tZM5uj"
      },
      "source": [
        "grid2 = RandomizedSearchCV(estimator= model_Adamax, cv=KFold(5), param_distributions=param_grid, verbose=20,  n_iter=10)"
      ],
      "id": "h3udy1tZM5uj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cASbuCnvM52o"
      },
      "source": [
        "grid_result_AM = grid2.fit(X_train, y_train, callbacks=callbacks_list,\n",
        "                            validation_data=(X_val, y_val))"
      ],
      "id": "cASbuCnvM52o",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14ay9vQ9k_fq"
      },
      "source": [
        "grid_result_AM.best_params_"
      ],
      "id": "14ay9vQ9k_fq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsqtbBgRk_f0"
      },
      "source": [
        "best_AM = grid_result_AM.best_estimator_"
      ],
      "id": "rsqtbBgRk_f0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-xY8MckA-wu"
      },
      "source": [
        "best_AM.model.save('/content/drive/MyDrive/ML_NN/adamax')"
      ],
      "id": "M-xY8MckA-wu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_u5iLB9Gng4Y"
      },
      "source": [
        "tunedAdamax = keras.models.load_model(\"/content/drive/MyDrive/ML_NN/adamax\")"
      ],
      "id": "_u5iLB9Gng4Y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ruUw_uQbcD0a"
      },
      "source": [
        "tunedAdamax.summary()"
      ],
      "id": "ruUw_uQbcD0a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0MBKT0tEL_P"
      },
      "source": [
        "historyAdamax = tunedAdamax.fit(X_train, y_train,\n",
        "  verbose=1, callbacks=callbacks_list,\n",
        "  validation_data=(X_val, y_val),\n",
        "  epochs = 30)"
      ],
      "id": "O0MBKT0tEL_P",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0M1OZ7ujEaWn"
      },
      "source": [
        "pd.DataFrame(historyAdamax.history).plot(figsize=(8, 5))\n",
        "plt.grid(True)\n",
        "plt.gca().set_ylim(0, 1)\n",
        "plt.show()"
      ],
      "id": "0M1OZ7ujEaWn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8SM3Lg6uPJD"
      },
      "source": [
        "tunedAdamax.evaluate(X_train, y_train)\n",
        "tunedAdamax.evaluate(X_test, y_test)"
      ],
      "id": "W8SM3Lg6uPJD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4lmOFTduPJQ"
      },
      "source": [
        "y_pred = tunedAdamax.predict(X_test)\n",
        "print(zo_loss(y_test, y_pred))"
      ],
      "id": "w4lmOFTduPJQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBrHUH5ABz7-"
      },
      "source": [
        "# Convolutional Neural Network"
      ],
      "id": "eBrHUH5ABz7-"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsOFb31vtvaa"
      },
      "source": [
        "### VGG16 Convolutional Network"
      ],
      "id": "ZsOFb31vtvaa"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dOp58NB5m4bb"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(input_shape=[32,32,3],filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
        "model.add(Conv2D(filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
        "model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
        "model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
        "model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
        "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
        "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(units=4096,activation=\"relu\"))\n",
        "model.add(Dense(units=4096,activation=\"relu\"))\n",
        "model.add(Dense(units=10, activation=\"softmax\"))"
      ],
      "id": "dOp58NB5m4bb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzYMCgBMq4WT"
      },
      "source": [
        "model.summary()"
      ],
      "id": "GzYMCgBMq4WT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqpBe6e8ohWu"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adamax',\n",
        "              metrics=['accuracy'])"
      ],
      "id": "rqpBe6e8ohWu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-TcPTiNohW6"
      },
      "source": [
        "history = model.fit(X_train,y_train,\n",
        "        epochs=20,\n",
        "        validation_data=(X_val, y_val),\n",
        "        verbose=1, shuffle=True)"
      ],
      "id": "j-TcPTiNohW6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVAA2LNfohW7"
      },
      "source": [
        "model.evaluate(X_train, y_train)\n",
        "model.evaluate(X_test, y_test)"
      ],
      "id": "bVAA2LNfohW7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBV_3XHrohW8"
      },
      "source": [
        "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
        "plt.grid(True)\n",
        "plt.gca().set_ylim(0, 1)\n",
        "plt.show()"
      ],
      "id": "WBV_3XHrohW8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yh0cV37GohW8"
      },
      "source": [
        "y_pred = model.predict(X_test)\n",
        "print(zo_loss(y_test, y_pred))"
      ],
      "id": "yh0cV37GohW8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5eMf5e4wkvE"
      },
      "source": [
        "### Res-Net 34"
      ],
      "id": "i5eMf5e4wkvE"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJfJ4P0Mygjk"
      },
      "source": [
        "class ResidualUnit(keras.layers.Layer):\n",
        "  def __init__(self, filters, strides=1, activation=\"relu\", **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.activation = keras.activations.get(activation)\n",
        "    self.main_layers = [\n",
        "      keras.layers.Conv2D(filters, 3, strides=strides, padding=\"same\", use_bias=False), \n",
        "      keras.layers.BatchNormalization(),\n",
        "      self.activation, keras.layers.Conv2D(filters, 3, strides=1, padding=\"same\", use_bias=False), keras.layers.BatchNormalization()]\n",
        "    self.skip_layers = []\n",
        "    if strides > 1:\n",
        "      self.skip_layers = [keras.layers.Conv2D(filters, 1, strides=strides, padding=\"same\", use_bias=False), keras.layers.BatchNormalization()]\n",
        "  def call(self, inputs):\n",
        "    Z = inputs\n",
        "    for layer in self.main_layers:\n",
        "      Z = layer(Z)\n",
        "    skip_Z = inputs\n",
        "    for layer in self.skip_layers:\n",
        "      skip_Z = layer(skip_Z)\n",
        "    return self.activation(Z + skip_Z)"
      ],
      "id": "pJfJ4P0Mygjk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "najXi2TZwuUx"
      },
      "source": [
        "model = keras.models.Sequential()\n",
        "model.add(keras.layers.Conv2D(64, 7, strides=2, input_shape=[32, 32, 3],\n",
        "                              padding=\"same\", use_bias=False))\n",
        "model.add(keras.layers.BatchNormalization())\n",
        "model.add(keras.layers.Activation(\"relu\"))\n",
        "model.add(keras.layers.MaxPool2D(pool_size=3, strides=2, padding=\"same\"))\n",
        "prev_filters = 64\n",
        "for filters in [64] * 3 + [128] * 4 + [256] * 6 + [512] * 3:\n",
        "  strides = 1 if filters == prev_filters else 2\n",
        "  model.add(ResidualUnit(filters, strides=strides))\n",
        "  prev_filters = filters\n",
        "model.add(keras.layers.GlobalAvgPool2D())\n",
        "model.add(keras.layers.Flatten())\n",
        "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
        "model.summary()"
      ],
      "id": "najXi2TZwuUx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7m3aYb4zTf5"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adamax',\n",
        "              metrics=['accuracy'])"
      ],
      "id": "l7m3aYb4zTf5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRRTtGRMzTgd"
      },
      "source": [
        "history = model.fit(X_train,y_train,\n",
        "        epochs=20,\n",
        "        validation_data=(X_val, y_val),\n",
        "        verbose=1, shuffle=True)"
      ],
      "id": "BRRTtGRMzTgd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JKas610zTgg"
      },
      "source": [
        "model.evaluate(X_train, y_train)\n",
        "model.evaluate(X_test, y_test)"
      ],
      "id": "1JKas610zTgg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHK_gSevzTgi"
      },
      "source": [
        "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
        "plt.grid(True)\n",
        "plt.gca().set_ylim(0, 1)\n",
        "plt.show()"
      ],
      "id": "bHK_gSevzTgi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLWGNkFnzTgk"
      },
      "source": [
        "y_pred = model.predict(X_test)\n",
        "print(zo_loss(y_test, y_pred))"
      ],
      "id": "VLWGNkFnzTgk",
      "execution_count": null,
      "outputs": []
    }
  ]
}